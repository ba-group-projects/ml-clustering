surplusPeak[i,p]=cv[i,2]-p
}
}
# Viewing a part of data in surplusPeak
# the first ten client's surpluses for Peak time slot if pPeak Price p=50, 51, 52, ..., 60.
colnames(surplusPeak)=paste0("p=",1:maxprice)
head(surplusPeak,5)
for (p in 1:maxprice){
demandNonPeak[p]=sum((maxsurplusNonPeak>surplusPeak[,p])*(maxsurplusNonPeak>=0))
demandPeak[p]=sum((surplusPeak[,p]>=maxsurplusNonPeak)*(surplusPeak[,p]>=0))
revenue[p]=non.peak.price*demandNonPeak[p]+p*demandPeak[p]
}
# Plotting NonPeak Demand vs Peak Period Price
xaxis=1:maxprice
plot(xaxis,demandNonPeak,pch = 16, type="s",col="blue", las=1, xaxt="n",
xlab="Price for Peak Period",ylab="Non-Peak Period Demand")
xticks <- seq(0, maxprice, by=1)
axis(side = 1, at = xticks)
# Plotting Peak Demand vs Peak Period Price
xaxis=1:maxprice
plot(xaxis,demandPeak,pch = 16, type="s",col="blue", las=1, xaxt="n",
xlab="Price for Peak Period (Time slot 5-9pm)",ylab="Peak Period Demand")
xticks <- seq(0, maxprice, by=1)
axis(side = 1, at = xticks)
# Plotting Revenue vs Peak Period Price
xaxis=1:maxprice
plot(xaxis,revenue,pch = 16, type="s",col="blue", las=1, xaxt="n",
xlab="Price for Peak Period",ylab="Total Revenue")
xticks <- seq(0, maxprice, by=1)
axis(side = 1, at = xticks)
revenueBest=max(revenue[non.peak.price:maxprice])
priceBest=which(revenue == revenueBest)
axis(side = 1, at = priceBest)
lines(c(priceBest,priceBest),c(0, revenueBest),lty=2)
axis(side = 2, at = round(revenueBest,3),las=1)
lines(c(0,priceBest),c(revenueBest, revenueBest),lty=2)
print(paste("When other periods have a base price of 7, the optimal price for the peak time is:",priceBest))
print(paste('In this case, the best revenue is',revenueBest/N *total.drivers))
# calculate the demand in peak time when peak price is 7
cal.total.emission = function(price){
demand.perk.opt = demandPeak[price]
# calculate the demand in non peak time when peak price is 7
demand.non.peak.opt = demandNonPeak[price]
# calculate the number of drivers who want to pay for the price in the sample in non peak period
total.driver.WTP.peak = demand.perk.opt/N*total.drivers
# calculate the number of drivers who want to pay for the price in the sample in non peak period
total.driver.WTP.non.peak = demand.non.peak.opt/N*total.drivers
# calculate the average speed in peak period
average.speed.peak = cal.average.speed(total.driver.WTP.peak)
# calculate the average speed in non peak period
average.speed.non.peak = cal.average.speed(total.driver.WTP.non.peak)
# calculate the emission per car in peak period
emission.per.car.peak = cal.emission.per.car(average.speed.peak)
# calculate the emission per car in non peak period
emission.per.car.non.peak = cal.emission.per.car(average.speed.non.peak)
# calculate the total emission in peak period
emission.total.peak = emission.per.car.peak * total.driver.WTP.peak
# calculate the total emission in non peak period
emission.total.non.peak = emission.per.car.non.peak * total.driver.WTP.non.peak
# emission total
emission.total = emission.total.peak + emission.total.non.peak
return(emission.total)
}
total.emisssion = cal.total.emission(priceBest)
print(paste('The total emission is this strategy is',round(total.emisssion,2)))
# calculate the total revenue based on the sample
revenue.total = revenue / N *total.drivers
# get the range of peak hour price we need to go through
price.range = which(revenue.total>=1100000)
emission.array = rep(0,length(price.range))
for (i in price.range){
emission= cal.total.emission( price = i)
emission.array[i-price.range[1]+1] = emission
}
price.least.emission =  which(emission.array== min(emission.array))
print(paste("The price picked for minimizing the emissions as well as ensure the total revenue is above 1.1 million is", price.least.emission+price.range[1]-1))
# Finding optimal revenue by optimization
library("nloptr")
# Differentiated Prices
eval_f <- function(x){
price = x
emission = cal.total.emission(price)
objfunction = emission
return(objfunction)
}
eval_g_ineq <- function(x) {
price = x
NonPeakDemand=demandNonPeak[x]
PeakDemand=demandPeak[x]
non.peak.revenue  =  NonPeakDemand/N*total.drivers*non.peak.price
peak.revenue = PeakDemand/N*total.drivers*x
total.revenue = non.peak.revenue + peak.revenue
constraint <- c(-NonPeakDemand,
-PeakDemand,
1100000 - total.revenue)
return(constraint)
}
x0 <- 1
# lower and upper bounds of control
lb <- c(1)
ub <- c(maxprice)
opts <- list( "algorithm" = "NLOPT_LN_COBYLA",
"xtol_rel" = 1.0e-9,
"maxeval" = 1000)
result <- nloptr(x0=x0,eval_f=eval_f,lb=lb,ub=ub,
eval_g_ineq=eval_g_ineq,opts=opts)
# print(result)
priceOpt<-result$solution
RevenueOpt<- -result$objective
eval_f(1)
eval_g_ineq(1)
345/N*total.drivers
# Finding optimal revenue by optimization
library("nloptr")
# Differentiated Prices
eval_f <- function(x){
price = x
emission = cal.total.emission(price)
objfunction = emission
return(objfunction)
}
eval_g_ineq <- function(x) {
price = x
NonPeakDemand=demandNonPeak[x]
PeakDemand=demandPeak[x]
non.peak.revenue  =  NonPeakDemand/N*total.drivers*non.peak.price
peak.revenue = PeakDemand/N*total.drivers*x
total.revenue = non.peak.revenue + peak.revenue
constraint <- c(-NonPeakDemand,
-PeakDemand,
1100000 - total.revenue)
return(constraint)
}
x0 <- 1
# lower and upper bounds of control
lb <- 1
ub <- maxprice
opts <- list( "algorithm" = "NLOPT_LN_COBYLA",
"xtol_rel" = 1.0e-9,
"maxeval" = 1000)
result <- nloptr(x0=x0,eval_f=eval_f,lb=lb,ub=ub,
eval_g_ineq=eval_g_ineq,opts=opts)
# print(result)
priceOpt<-result$solution
emission<- result$objective
eval_f(13)
eval_g_ineq(13)
-141/N*total.drivers
-87/N*total.drivers
integer(01.2)
integer(1.2)
integer(1.4)
integer(1.5)
as.integer(1.5)
as.integer(1.5)
as.integer(1.6)
# import libraries
library(dplyr)
library(GGally)
library(corrplot)
# import data
data.ori = read.csv("customer-personality.csv")
# summary of data
str(data.ori)
# check for NA
sum(is.na(data.ori))
# check which columns contain NA
apply(is.na(data.ori), 2, which)
# remove NA rows
data.clean = data.ori[which(!is.na(data.ori$Income)),]
str(data.clean)
sum(is.na(data.clean))
# remove outliers from income
data.clean <- data.clean[!(data.clean$Income >=200000),]
# filter numerical columns
numerical.data = data.clean[, c(2,5:7,9:ncol(data.ori))]
str(numerical.data)
# plot correlation matrix
corrplot(cor(numerical.data), tl.col = "black", diag = FALSE,
method = 'number', type = 'upper')
# # inefficient to use this, too much data
# ggpairs(numerical.data)#, aes(color = class, alpha = 0.5))
# converting 'Education' and 'Marital status' variables to categorical variables
## Education
data.clean$Education[data.clean$Education == "Basic"] <- "1"
data.clean$Education[data.clean$Education == "Graduation"] <- "2"
data.clean$Education[data.clean$Education == "2n Cycle"] <- "3"
data.clean$Education[data.clean$Education == "Master"] <- "3"
data.clean$Education[data.clean$Education == "PhD"] <- "4"
data.clean$Education <- as.numeric(data.clean$Education)
## Marital Status - Merging YOLO, Alone, and Absurd to add to single
data.clean$Marital_Status[data.clean$Marital_Status == "Absurd"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Alone"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "YOLO"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Single"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Together"] <- "2"
data.clean$Marital_Status[data.clean$Marital_Status == "Married"] <- "3"
data.clean$Marital_Status[data.clean$Marital_Status == "Divorced"] <- "4"
data.clean$Marital_Status[data.clean$Marital_Status == "Widow"] <- "5"
data.clean$Marital_Status <- as.numeric(data.clean$Marital_Status)
# Adding Age variable (EXPLAIN)
data.clean$Age <- (2014 - data.clean$Year_Birth)
# Figuring out 'Dt_Customer' dataset
data.clean$Dt_Customer <- as.Date.numeric(data.clean$Dt_Customer)
######## Factor Analysis ########
### determine the number of factors of FA
# We want to identify the component of the correlation structure
# Remove Dt_customer for now
data.clean$Dt_Customer <- NULL
fa.cor = cor(data.clean)
# import libraries
library(dplyr)
library(GGally)
library(corrplot)
# import data
data.ori = read.csv("customer-personality.csv")
# summary of data
str(data.ori)
# check for NA
sum(is.na(data.ori))
# check which columns contain NA
apply(is.na(data.ori), 2, which)
# remove NA rows
data.clean = data.ori[which(!is.na(data.ori$Income)),]
str(data.clean)
sum(is.na(data.clean))
# remove outliers from income
data.clean <- data.clean[!(data.clean$Income >=200000),]
# filter numerical columns
numerical.data = data.clean[, c(2,5:7,9:ncol(data.ori))]
str(numerical.data)
# plot correlation matrix
corrplot(cor(numerical.data), tl.col = "black", diag = FALSE,
method = 'number', type = 'upper')
# # inefficient to use this, too much data
# ggpairs(numerical.data)#, aes(color = class, alpha = 0.5))
# converting 'Education' and 'Marital status' variables to categorical variables
## Education
data.clean$Education[data.clean$Education == "Basic"] <- "1"
data.clean$Education[data.clean$Education == "Graduation"] <- "2"
data.clean$Education[data.clean$Education == "2n Cycle"] <- "3"
data.clean$Education[data.clean$Education == "Master"] <- "3"
data.clean$Education[data.clean$Education == "PhD"] <- "4"
data.clean$Education <- as.numeric(data.clean$Education)
## Marital Status - Merging YOLO, Alone, and Absurd to add to single
data.clean$Marital_Status[data.clean$Marital_Status == "Absurd"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Alone"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "YOLO"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Single"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Together"] <- "2"
data.clean$Marital_Status[data.clean$Marital_Status == "Married"] <- "3"
data.clean$Marital_Status[data.clean$Marital_Status == "Divorced"] <- "4"
data.clean$Marital_Status[data.clean$Marital_Status == "Widow"] <- "5"
data.clean$Marital_Status <- as.numeric(data.clean$Marital_Status)
# Adding Age variable (EXPLAIN)
data.clean$Age <- (2014 - data.clean$Year_Birth)
# Figuring out 'Dt_Customer' dataset
data.clean$Dt_Customer <- as.Date.numeric(data.clean$Dt_Customer)
######## Factor Analysis ########
### determine the number of factors of FA
# We want to identify the component of the correlation structure
# Remove Dt_customer for now
data.clean$Dt_Customer <- NULL
fa.cor = cor(data.clean)
setwd("~/OneDrive/Documents/MyOversea/Cass Study/machine learning/machine-learning-gp2")
data.
# import libraries
library(dplyr)
library(GGally)
library(corrplot)
# import data
data.ori = read.csv("customer-personality.csv")
# summary of data
str(data.ori)
# check for NA
sum(is.na(data.ori))
# check which columns contain NA
apply(is.na(data.ori), 2, which)
# remove NA rows
data.clean = data.ori[which(!is.na(data.ori$Income)),]
str(data.clean)
sum(is.na(data.clean))
# remove outliers from income
data.clean <- data.clean[!(data.clean$Income >=200000),]
# filter numerical columns
numerical.data = data.clean[, c(2,5:7,9:ncol(data.ori))]
str(numerical.data)
# plot correlation matrix
corrplot(cor(numerical.data), tl.col = "black", diag = FALSE,
method = 'number', type = 'upper')
# # inefficient to use this, too much data
# ggpairs(numerical.data)#, aes(color = class, alpha = 0.5))
# converting 'Education' and 'Marital status' variables to categorical variables
## Education
data.clean$Education[data.clean$Education == "Basic"] <- "1"
data.clean$Education[data.clean$Education == "Graduation"] <- "2"
data.clean$Education[data.clean$Education == "2n Cycle"] <- "3"
data.clean$Education[data.clean$Education == "Master"] <- "3"
data.clean$Education[data.clean$Education == "PhD"] <- "4"
data.clean$Education <- as.numeric(data.clean$Education)
## Marital Status - Merging YOLO, Alone, and Absurd to add to single
data.clean$Marital_Status[data.clean$Marital_Status == "Absurd"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Alone"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "YOLO"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Single"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Together"] <- "2"
data.clean$Marital_Status[data.clean$Marital_Status == "Married"] <- "3"
data.clean$Marital_Status[data.clean$Marital_Status == "Divorced"] <- "4"
data.clean$Marital_Status[data.clean$Marital_Status == "Widow"] <- "5"
data.clean$Marital_Status <- as.numeric(data.clean$Marital_Status)
# Adding Age variable (EXPLAIN)
data.clean$Age <- (2014 - data.clean$Year_Birth)
# Figuring out 'Dt_Customer' dataset
data.clean$Dt_Customer <- as.Date.numeric(data.clean$Dt_Customer)
######## Factor Analysis ########
### determine the number of factors of FA
# We want to identify the component of the correlation structure
# Remove Dt_customer for now
data.clean$Dt_Customer <- NULL
fa.cor = cor(data.clean)
data.clean
colnames(data.clean)
fa.res = factanal(x = data.clean[,2:7], factors = 7, rotation = "none") # factor = 4 because of our eigenvalues easlier
fa.res
fa.res = factanal(x = data.clean[,2:7], factors = 3, rotation = "none") # factor = 4 because of our eigenvalues easlier
fa.res
fa.res = factanal(x = data.clean, factors = 2, rotation = "promax")
# promax belongs to the oblique rotation?
print(fa.res, cut = 0.2)
colnames(data.clean)
fa.res = factanal(x = data.clean[,8:14], factors = 3, rotation = "none") # factor = 4 because of our eigenvalues easlier
fa.res
fa.res = factanal(x = data.clean[,8:14], factors = 3, rotation = "promax")
# promax belongs to the oblique rotation?
print(fa.res, cut = 0.2)
# import libraries
library(dplyr)
library(GGally)
library(corrplot)
# import data
data.ori = read.csv("customer-personality.csv")
# summary of data
str(data.ori)
# check for NA
sum(is.na(data.ori))
# check which columns contain NA
apply(is.na(data.ori), 2, which)
# remove NA rows
data.clean = data.ori[which(!is.na(data.ori$Income)),]
str(data.clean)
sum(is.na(data.clean))
# remove outliers from income
data.clean <- data.clean[!(data.clean$Income >=200000),]
# filter numerical columns
numerical.data = data.clean[, c(2,5:7,9:ncol(data.ori))]
str(numerical.data)
# plot correlation matrix
corrplot(cor(numerical.data), tl.col = "black", diag = FALSE,
method = 'number', type = 'upper')
# # inefficient to use this, too much data
# ggpairs(numerical.data)#, aes(color = class, alpha = 0.5))
# converting 'Education' and 'Marital status' variables to categorical variables
## Education
data.clean$Education[data.clean$Education == "Basic"] <- "1"
data.clean$Education[data.clean$Education == "Graduation"] <- "2"
data.clean$Education[data.clean$Education == "2n Cycle"] <- "3"
data.clean$Education[data.clean$Education == "Master"] <- "3"
data.clean$Education[data.clean$Education == "PhD"] <- "4"
data.clean$Education <- as.numeric(data.clean$Education)
## Marital Status - Merging YOLO, Alone, and Absurd to add to single
data.clean$Marital_Status[data.clean$Marital_Status == "Absurd"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Alone"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "YOLO"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Single"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Together"] <- "2"
data.clean$Marital_Status[data.clean$Marital_Status == "Married"] <- "3"
data.clean$Marital_Status[data.clean$Marital_Status == "Divorced"] <- "4"
data.clean$Marital_Status[data.clean$Marital_Status == "Widow"] <- "5"
data.clean$Marital_Status <- as.numeric(data.clean$Marital_Status)
# Adding Age variable (EXPLAIN)
data.clean$Age <- (2014 - data.clean$Year_Birth)
# Figuring out 'Dt_Customer' dataset
data.clean$Dt_Customer <- as.Date.numeric(data.clean$Dt_Customer)
######## Factor Analysis ########
### determine the number of factors of FA
# We want to identify the component of the correlation structure
# Remove Dt_customer for now
data.clean$Dt_Customer <- NULL
fa.cor = cor(data.clean)
# looking at fa.cor
# let's look at eigenvector of the correlation to see how many factors we use
fa.eigen = eigen(fa.cor)
fa.eigen$values
# in decreasing values
# higher = more important
# rule of thuumb to see the number of variables we should use? Do the sum
sum(fa.eigen$values)
cumsum(fa.eigen$values)/19
# seeing this, using the first 4, we can explain 79% of the data
# draw screen plot for better visualisation
# use the scree plot
plot(fa.eigen$values, type = "b", ylab = "Eigenvalues", xlab = "Factor") # we choose 4
# plot(cumsum(fa.eigen$values)/17, type = "b", ylab = "Eigenvalues", xlab = "Factor") # we choose 4
##############################################
### factor analysis
# Choosing 6 as it explains 71.4% of the data
fa.res = factanal(x = data.clean, factors = 7, rotation = "none") # factor = 4 because of our eigenvalues easlier
fa.res
#uniqueness = epsilon values
# loadings
# We have proportion variance explained by these loadings
# hypothesis test to see whether 4 factors are sufficient
# here, our p value is high therefore we don't reject that 4 factors are sufficient
##############################################
### factor rotation
fa.res = factanal(x = data.clean, factors = 6, rotation = "promax")
# promax belongs to the oblique rotation?
print(fa.res, cut = 0.2)
# we only show the factors with a loading of higher than 0.2
# therefore only selecting the variables with large loading
# you can see that paragraph, sentence and wordm is with factor 1
##############################################
### factor scores
fa.res = factanal(x = data.clean, factors = 7, rotation = "promax", scores = "Bartlett")
head(fa.res$scores)
summary(lm(Factor2 ~ Factor1, data = as.data.frame(fa.res$scores)))
####### PCA ########
# # briefly examine the data
# apply(data.clean, 2, mean)
# apply(data.clean, 2, var)
# apply PCA
# Apply PCR to data
pr.out = prcomp(data.clean, scale = TRUE)
# # trying out PCA with first 7 columns
# pr.out = prcomp(data.freq, scale = TRUE)
names(pr.out)
# have a look at the output
pr.out$center
pr.out$scale
pr.out$rotation
# get the PC vector
dim(pr.out$x)
# plot the PCs
biplot(pr.out, scale = 0,cex=0.5)
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale =0,cex=0.5)
# check the variance explained by each PC
pr.out$sdev
pr.var = pr.out$sdev ^2
pr.var
# proportion of variance explained
pve = pr.var/sum(pr.var)
pve
cumsum(pve)
plot(pve, xlab = " Principal Component", ylab = "Proportion of
Variance Explained", ylim = c(0,1), type = "b")
plot(cumsum(pve), xlab = "Principal Component", ylab ="
Cumulative Proportion of Variance Explained", ylim = c(0,1),
type = "b")
# Plotting PC1 and PC2
PC1 <- pr.out$rotation[,1]
pca_1_2 <- data.frame(pr.out$x[, 1:2])
plot(pca_1_2[,1], pca_1_2[,2])
"
PCA -
from TDS:
This plot clearly shows how instead of the 8 columns given to us in the dataset,
only two were enough to understand we had three different types of pizzas,
thus making PCA a successful analytical tool to reduce high-dimensional
data into a lower one for modelling and analytical purposes.
our_results
PCA may not be a suitable task, as it is unable for us to clearly understand the
number and identity of segments in our customer dataset.
"
# Plotting importance of each variable
PC1 <- pr.out$rotation[,1]
PC1_scores <- abs(PC1)
PC1_scores_ordered <- sort(PC1_scores, decreasing = TRUE)
names(PC1_scores_ordered)
# Plotting 3d
# install.packages("scatterplot3d")
#
# library(scatterplot3d)
pca_1to3 <- data.frame(pr.out$x[, 1:3])
scatterplot3d(pca_1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=60)
############################################################
############ Independent component analysis ###############
##### get source matrix
S = cbind(sin((1:1000)/20), rep((((1:200)-100)/100), 5))
##### get mixing matrix
M = matrix(c(0.291, 0.6557, -0.5439, 0.5572), 2, 2)
##### plot ource signals
par(mfrow = c(1, 2))
plot(1:1000, S[,1], type = "l",xlab = "source 1",ylab="",cex.lab=1.5,cex.axis=1.5,lwd=2)
plot(1:1000, S[,2], type = "l", xlab = "source 2",ylab="",cex.lab=1.5,cex.axis=1.5,lwd=2)
##### mix the source signals to get the observed signals X
X = S %*% M
par(mfrow = c(1, 2))
plot(1:1000, X[,1], type = "l",xlab = "observation 1",ylab="",cex.lab=1.5,cex.axis=1.5,lwd=2)
plot(1:1000, X[,2], type = "l", xlab = "observation 2",ylab="",cex.lab=1.5,cex.axis=1.5,lwd=2)
##### apply ICA
library(fastICA)
set.seed(20)
latent = fastICA(X, 2, fun = "logcosh", alpha = 1,
row.norm = FALSE, maxit = 200,
tol = 0.0001, verbose = TRUE)
latent
