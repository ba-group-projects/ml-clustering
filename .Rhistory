}
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19,
xlab="Z1",ylab="Z2")
model = keras_model_sequential()
model %>%
layer_dense(units = 2, activation = 'linear', input_shape = ncol(nci.data),name = "subspace") %>%
layer_dense(units = ncol(nci.data), activation = 'linear')
summary(model)
model %>% compile(
loss = 'mean_squared_error',
optimizer = "adam"
)
history <- model %>% fit(
as.matrix(nci.data), as.matrix(nci.data),
epochs = 1000
)
subspace <- keras_model(inputs = model$input, outputs = get_layer(model, "subspace")$output)
projection1 <- predict(subspace, nci.data)
plot(projection1, col=Cols(nci.labs), pch=19,
xlab="W1",ylab="W2")
yes
#############################################################
####### installation #######################################
### install tensorflow
#install.packages("tensorflow")
#library(tensorflow)
install_tensorflow() #this step takes a while
#############################################################
####### installation #######################################
### install tensorflow
install.packages("tensorflow")
install.packages("tensorflow")
library(keras)
library(ISLR)
######get gene data
nci.labs=NCI60$labs
nci.data=NCI60$data
####################################
######PCA
pr.out=prcomp(nci.data, scale=TRUE)
ols=function(vec){
cols=rainbow(length(unique(vec)))
return(cols[as.numeric(as.factor(vec))])
}
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19,
xlab="Z1",ylab="Z2")
model = keras_model_sequential()
model %>%
layer_dense(units = 2, activation = 'linear', input_shape = ncol(nci.data),name = "subspace") %>%
layer_dense(units = ncol(nci.data), activation = 'linear')
summary(model)
model %>% compile(
loss = 'mean_squared_error',
optimizer = "adam"
)
library(dplyr)
library(GGally)
library(corrplot)
# import data
data.ori = read.csv("customer-personality.csv")
# summary of data
str(data.ori)
# check for NA
sum(is.na(data.ori))
# check which columns contain NA
apply(is.na(data.ori), 2, which)
# remove NA rows
data.clean = data.ori[which(!is.na(data.ori$Income)),]
str(data.clean)
sum(is.na(data.clean))
# remove outliers from income
data.clean <- data.clean[!(data.clean$Income >=200000),]
# filter numerical columns
numerical.data = data.clean[, c(2,5:7,9:ncol(data.ori))]
str(numerical.data)
# plot correlation matrix
corrplot(cor(numerical.data), tl.col = "black", diag = FALSE,
method = 'number', type = 'upper')
# # inefficient to use this, too much data
# ggpairs(numerical.data)#, aes(color = class, alpha = 0.5))
# converting 'Education' and 'Marital status' variables to categorical variables
## Education
data.clean$Education[data.clean$Education == "Basic"] <- "1"
data.clean$Education[data.clean$Education == "Graduation"] <- "2"
data.clean$Education[data.clean$Education == "2n Cycle"] <- "3"
data.clean$Education[data.clean$Education == "Master"] <- "3"
data.clean$Education[data.clean$Education == "PhD"] <- "4"
data.clean$Education <- as.numeric(data.clean$Education)
## Marital Status - Merging YOLO, Alone, and Absurd to add to single
data.clean$Marital_Status[data.clean$Marital_Status == "Absurd"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Alone"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "YOLO"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Single"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Together"] <- "2"
data.clean$Marital_Status[data.clean$Marital_Status == "Married"] <- "3"
data.clean$Marital_Status[data.clean$Marital_Status == "Divorced"] <- "4"
data.clean$Marital_Status[data.clean$Marital_Status == "Widow"] <- "5"
data.clean$Marital_Status <- as.numeric(data.clean$Marital_Status)
# Adding Age variable (EXPLAIN)
data.clean$Age <- (2014 - data.clean$Year_Birth)
# Figuring out 'Dt_Customer' dataset
data.clean$Dt_Customer <- as.Date.numeric(data.clean$Dt_Customer)
# TODO
# Remove Dt_customer for now
data.clean$Dt_Customer <- NULL
### determine the number of factors of FA
fa.cor = cor(data.clean)
fa.eigen = eigen(fa.cor)
fa.eigen$values
sum(fa.eigen$values)
# Cumulative sum of eigenvalues
cumsum(fa.eigen$values)/19
# use the scree plot
plot(fa.eigen$values, type = "b", ylab = "Eigenvalues", xlab = "Factor") # we choose 4
### Factor Analysis
# Choosing 6 as it explains 71.4% of the data
fa.res = factanal(x = data.clean, factors = 7, rotation = "none") # factor = 4 because of our eigenvalues easlier
fa.res
pr.out = prcomp(data.clean, scale = TRUE)
# PCA Rotation Output
pr.out$rotation
# plot the PCs
biplot(pr.out, scale = 0,cex=0.5)
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale =0,cex=0.5)
# check the variance explained by each PC
pr.var = pr.out$sdev ^2
# proportion of variance explained
pve = pr.var/sum(pr.var)
cumsum(pve)
plot(pve, xlab = " Principal Component", ylab = "Proportion of
Variance Explained", ylim = c(0,1), type = "b")
## Test 1 - Taking only two components
# Plotting PC1 and PC2
PC1 <- pr.out$rotation[,1]
pca_1_2 <- data.frame(pr.out$x[, 1:2])
plot(pca_1_2[,1], pca_1_2[,2])
# Applying kmeans to PCA
library(cluster)
# Sample
model=kmeans(pca_1_2, 2, nstart = 10)
clusplot(pca_1_2,model$cluster, main="3 Cluster")
sil <- silhouette(model$cluster, dist(pca_1_2))
fviz_silhouette(sil)
mean(sil[,3])
# Sample
model=kmeans(pca_1_2, 2, nstart = 10)
clusplot(pca_1_2,model$cluster, main="3 Cluster")
sil <- silhouette(model$cluster, dist(pca_1_2))
fviz_silhouette(sil)
mean(sil[,3])
plot(pca_1_2[,1], pca_1_2[,2])
"
PCA -
from TDS:
This plot clearly shows how instead of the 8 columns given to us in the dataset,
only two were enough to understand we had three different types of pizzas,
thus making PCA a successful analytical tool to reduce high-dimensional
data into a lower one for modelling and analytical purposes.
our_results
PCA may not be a suitable task, as it is unable for us to clearly understand the
number and identity of segments in our customer dataset.
- need to dispute on normal distribution
"
plot(pca_1_2[,1], pca_1_2[,2])
par(mfrow = c(1, 1))
plot(pca_1_2[,1], pca_1_2[,2])
model=kmeans(pca_1_2, 2, nstart = 10)
clusplot(pca_1_2,model$cluster, main="3 Cluster")
sil <- silhouette(model$cluster, dist(pca_1_2))
fviz_silhouette(sil)
mean(sil[,3])
# Test different silhouette scores
loop_data <- pca_1_2
score = c()
n_cluster = c()
for (i in 2:5) {
model=kmeans(loop_data, i, nstart = 10)
sil <- silhouette(model$cluster, dist(loop_data))
mean_sil <- mean(sil[,3])
score <- c(score, mean_sil)
n_cluster = c(n_cluster, i)
print(score)
}
# Table to show different silhouettte score
sil_table = cbind(n_cluster, score)
model=kmeans(pca_1_2, 2, nstart = 10)
clusplot(pca_1_2,model$cluster, main="2 Cluster")
plot(pca_1_2[,1], pca_1_2[,2], col=model$cluster)
model=kmeans(pca_1_2, 3, nstart = 10)
clusplot(pca_1_2,model$cluster, main="2 Cluster")
plot(pca_1_2[,1], pca_1_2[,2], col=model$cluster)
pca1to3 <- data.frame(pr.out$x[, 1:3])
model_3cluster=kmeans(pca1to3, 3, nstart = 10)
model_3cluster$cluster
scatterplot3d(pca_1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40)
library(scatterplot3d)
scatterplot3d(pca_1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40)
pca1to3 <- data.frame(pr.out$x[, 1:3])
scatterplot3d(pca1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40)
scatterplot3d(pca1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40,
col = model_3cluster$cluster)
model_3cluster$cluster
scatterplot3d(pca1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40,
col = model_3cluster$cluster)
biplot(pr.out, scale =0,cex=0.5, col.grid=model$cluster)
scatterplot3d(pca1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40,
col.grid = model_3cluster$cluster)
scatterplot3d(pca1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40,
col.lab = model_3cluster$cluster)
scatterplot3d(pca1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40,
col.axis = model_3cluster$cluster)
scatterplot3d(pca1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40,
col = model_3cluster$cluster)
scatterplot3d(pca1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40,
col = model_3cluster$cluster)
scatterplot3d(pca1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40,
col = "purple")
scatterplot3d(pca1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40,
color = "purple")
scatterplot3d(pca1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40,
color = model_3cluster$cluster)
plot(pca_1_2[,1], pca_1_2[,2], col=model$cluster)
View(sil_table)
View(sil_table)
cumsum(pve)
# Define the pca data and how many components
pca_data <- data.frame(pr.out$x[, 1:6])
# Test different silhouette scores
# loop_data <- pca_1_2
loop_data <- pca_data
score = c()
n_cluster = c()
for (i in 2:9) {
model=kmeans(loop_data, i, nstart = 10)
sil <- silhouette(model$cluster, dist(loop_data))
mean_sil <- mean(sil[,3])
score <- c(score, mean_sil)
n_cluster = c(n_cluster, i)
}
# Table to show different silhouettte score
sil_table = cbind(n_cluster, score)
sil_table
import libraries
library(dplyr)
library(GGally)
library(corrplot)
# import data
data.ori = read.csv("customer-personality.csv")
# summary of data
str(data.ori)
# check for NA
sum(is.na(data.ori))
# check which columns contain NA
apply(is.na(data.ori), 2, which)
# remove NA rows
data.clean = data.ori[which(!is.na(data.ori$Income)),]
str(data.clean)
sum(is.na(data.clean))
# remove outliers from income
data.clean <- data.clean[!(data.clean$Income >=200000),]
# filter numerical columns
numerical.data = data.clean[, c(2,5:7,9:ncol(data.ori))]
str(numerical.data)
# plot correlation matrix
corrplot(cor(numerical.data), tl.col = "black", diag = FALSE,
method = 'number', type = 'upper')
# # inefficient to use this, too much data
# ggpairs(numerical.data)#, aes(color = class, alpha = 0.5))
# converting 'Education' and 'Marital status' variables to categorical variables
## Education
data.clean$Education[data.clean$Education == "Basic"] <- "1"
data.clean$Education[data.clean$Education == "Graduation"] <- "2"
data.clean$Education[data.clean$Education == "2n Cycle"] <- "3"
data.clean$Education[data.clean$Education == "Master"] <- "3"
data.clean$Education[data.clean$Education == "PhD"] <- "4"
data.clean$Education <- as.numeric(data.clean$Education)
## Marital Status - Merging YOLO, Alone, and Absurd to add to single
data.clean$Marital_Status[data.clean$Marital_Status == "Absurd"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Alone"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "YOLO"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Single"] <- "1"
data.clean$Marital_Status[data.clean$Marital_Status == "Together"] <- "2"
data.clean$Marital_Status[data.clean$Marital_Status == "Married"] <- "3"
data.clean$Marital_Status[data.clean$Marital_Status == "Divorced"] <- "4"
data.clean$Marital_Status[data.clean$Marital_Status == "Widow"] <- "5"
data.clean$Marital_Status <- as.numeric(data.clean$Marital_Status)
# Adding Age variable (EXPLAIN)
data.clean$Age <- (2014 - data.clean$Year_Birth)
# Figuring out 'Dt_Customer' dataset
data.clean$Dt_Customer <- as.Date.numeric(data.clean$Dt_Customer)
# TODO
# Remove Dt_customer for now
data.clean$Dt_Customer <- NULL
### determine the number of factors of FA
fa.cor = cor(data.clean)
fa.eigen = eigen(fa.cor)
fa.eigen$values
sum(fa.eigen$values)
# Cumulative sum of eigenvalues
cumsum(fa.eigen$values)/19
# use the scree plot
plot(fa.eigen$values, type = "b", ylab = "Eigenvalues", xlab = "Factor") # we choose 4
### Factor Analysis
# Choosing 6 as it explains 71.4% of the data
fa.res = factanal(x = data.clean, factors = 7, rotation = "none") # factor = 4 because of our eigenvalues easlier
fa.res
# Apply PCR to data
pr.out = prcomp(data.clean, scale = TRUE)
# PCA Rotation Output
pr.out$rotation
# plot the PCs
biplot(pr.out, scale = 0,cex=0.5)
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale =0,cex=0.5)
# check the variance explained by each PC
pr.var = pr.out$sdev ^2
# proportion of variance explained
pve = pr.var/sum(pr.var)
cumsum(pve)
plot(pve, xlab = " Principal Component", ylab = "Proportion of
Variance Explained", ylim = c(0,1), type = "b")
# Plotting importance of each variable
PC1 <- pr.out$rotation[,1]
PC1_scores <- abs(PC1)
PC1_scores_ordered <- sort(PC1_scores, decreasing = TRUE)
names(PC1_scores_ordered)
PC2 <- pr.out$rotation[,1]
PC2_scores <- abs(PC2)
PC2_scores_ordered <- sort(PC2_scores, decreasing = TRUE)
names(PC2_scores_ordered)
PC2 <- pr.out$rotation[,2]
PC2_scores <- abs(PC2)
PC2_scores_ordered <- sort(PC2_scores, decreasing = TRUE)
names(PC2_scores_ordered)
PC3 <- pr.out$rotation[,3]
PC3_scores <- abs(PC3)
PC3_scores_ordered <- sort(PC3_scores, decreasing = TRUE)
names(PC3_scores_ordered)
# Define the pca data and how many components
pca_data <- data.frame(pr.out$x[, 1:3])
# Applying kmeans to PCA
library(cluster)
model=kmeans(pca_data, 2, nstart = 10)
clusplot(pca_1_2,model$cluster, main="3 Cluster")
sil <- silhouette(model$cluster, dist(pca_1_2))
fviz_silhouette(sil)
mean(sil[,3])
model=kmeans(pca_data, 2, nstart = 10)
clusplot(pca_1_2,model$cluster, main="3 Cluster")
sil <- silhouette(model$cluster, dist(pca_data))
fviz_silhouette(sil)
mean(sil[,3])
# Test different silhouette scores
# loop_data <- pca_1_2
loop_data <- pca_data
score = c()
n_cluster = c()
for (i in 2:9) {
model=kmeans(loop_data, i, nstart = 10)
sil <- silhouette(model$cluster, dist(loop_data))
mean_sil <- mean(sil[,3])
score <- c(score, mean_sil)
n_cluster = c(n_cluster, i)
}
# Table to show different silhouettte score
sil_table = cbind(n_cluster, score)
sil_table
library(scatterplot3d)
pca1to3 <- data.frame(pr.out$x[, 1:3])
model_3cluster=kmeans(pca1to3, 3, nstart = 10)
scatterplot3d(pca1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40,
color = model_3cluster$cluster)
model_3cluster$cluster
# Define the pca data and how many components
pca_data <- data.frame(pr.out$x[, 1:5])
loop_data <- pca_data
score = c()
n_cluster = c()
for (i in 2:9) {
model=kmeans(loop_data, i, nstart = 10)
sil <- silhouette(model$cluster, dist(loop_data))
mean_sil <- mean(sil[,3])
score <- c(score, mean_sil)
n_cluster = c(n_cluster, i)
}
# Table to show different silhouettte score
sil_table = cbind(n_cluster, score)
sil_table
model=kmeans(pca_data, 3, nstart = 10)
model=kmeans(pca_data, 2, nstart = 10)
data.clean$Year_Birth <- NULL
data.clean$ID <- NULL
# Apply PCR to data
pr.out = prcomp(data.clean, scale = TRUE)
# PCA Rotation Output
pr.out$rotation
biplot(pr.out, scale = 0,cex=0.5)
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale =0,cex=0.5)
# check the variance explained by each PC
pr.var = pr.out$sdev ^2
# proportion of variance explained
pve = pr.var/sum(pr.var)
cumsum(pve)
plot(pve, xlab = " Principal Component", ylab = "Proportion of
Variance Explained", ylim = c(0,1), type = "b")
# check the variance explained by each PC
pr.var = pr.out$sdev ^2
pr.var
# Define the pca data and how many components
pca_data <- data.frame(pr.out$x[, 1:5])
# Applying kmeans to PCA
library(cluster)
loop_data <- pca_data
score = c()
n_cluster = c()
for (i in 2:9) {
model=kmeans(loop_data, i, nstart = 10)
sil <- silhouette(model$cluster, dist(loop_data))
mean_sil <- mean(sil[,3])
score <- c(score, mean_sil)
n_cluster = c(n_cluster, i)
}
# Table to show different silhouettte score
sil_table = cbind(n_cluster, score)
sil_table
PC1 <- pr.out$rotation[,1]
PC1_scores <- abs(PC1)
PC1_scores_ordered <- sort(PC1_scores, decreasing = TRUE)
names(PC1_scores_ordered)
# Plotting importance of each variable
PC2 <- pr.out$rotation[,2]
PC2_scores <- abs(PC2)
PC2_scores_ordered <- sort(PC2_scores, decreasing = TRUE)
names(PC2_scores_ordered)
PC3 <- pr.out$rotation[,3]
PC3_scores <- abs(PC3)
PC3_scores_ordered <- sort(PC3_scores, decreasing = TRUE)
names(PC3_scores_ordered)
pr.out
biplot(pr.out, scale =0,cex=0.5, col=model$cluster)
model=kmeans(pca_data, 2, nstart = 10)
clusplot(pca_data,model$cluster, main="2 Cluster")
plot(pca_data[,1], pca_data[,2], col=model$cluster)
scatterplot3d(pca1to3,
main="3D Scatter Plot",
xlab = "PCA1",
ylab = "PCA2",
zlab = "PCA3", angle=40,
color = model$cluster)
