---
title: "Customer segementation analysis "
header-includes:
- \usepackage{ggplot2}
- \usepackage{reshape}
- \usepackage{dplyr}
- \usepackage{GGally}
- \usepackage{corrplot} 
- \usepackage{lubridate} 
- \usepackage{forcats} 
- \usepackage{readr}
- \usepackage{reshape} 
- \usepackage{polycor} 
- \usepackage{EFA.dimensions} 
- \usepackage{fastICA}
- \usepackage{tidyr} 
- \usepackage{stringr}
- \usepackage{grid} 
- \usepackage{gridExtra}
- \usepackage{ica}
- \usepackage{factoextra}
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE,fig.width=6, fig.height=6,warning=FALSE)
```

# Question
Download the customer-personality.csv data from Moodle. This dataset contains information of customers that can make the company better understand the behaviour of their customers. Explore the dataset and report any interesting findings.

# Main Code

Import the required libraries
```{r import all libraries we need}
library(ggplot2)
library(reshape)
library(dplyr)
library(GGally)
library(corrplot)
library(lubridate)
library(forcats)
library(readr)
library(reshape)
library(polycor)
library(EFA.dimensions)
library(fastICA)
library(tidyr)
library(stringr)
library(grid)
library(gridExtra)
library(ica)
library(scatterplot3d)
library(factoextra)
```

## I. Preprocess the data

We are going to preprocess the data as follows:

1. Remove the row with missing values

2. Remove the outliers from income and birth year

3. Convert 'Education', 'Marital status', and 'Dt_Customer' variables to categorical variables

  - Create 3 categories for 'Education':
  
    "Basic" <- 1
    
    "Graduation" <- 2
    
    "2n Cycle" <- 3
    
    "Master" <- 3
    
    "PhD" <- 3
    

  - Create 2 categories for 'Marital status' based on whether they are single or together:
  
    "Absurd" <- 0
    
    "Alone" <- 0
    
    "YOLO" <- 0
    
    "Single" <- 0
    
    "Together" <- 1
    
    "Married" <- 1
    
    "Divorced" <- 0
    
    "Widow" <- 0
    

  - Create 3 categories for Dt_Customer by the year they joined
  
    "2012" <- 1
    
    "2013" <- 2
    
    "2014" <- 3
    

4. Add age variable

5. Filter the data which we need and turn them into numeric variables


```{r proprocessing the data}
# import data
data.ori = read.csv("customer-personality.csv")

# summary of data
str(data.ori)
```

```{r remove the row with missing values}

# check for NA
sum(is.na(data.ori))

# check which columns contain NA
apply(is.na(data.ori), 2, which)

# remove NA rows
data.clean = data.ori[which(!is.na(data.ori$Income)),]
str(data.clean)
sum(is.na(data.clean))
```

```{r remove the outliers from income and birth year}

# remove outliers from income and birth year
## income
data.clean <- data.clean[!(data.clean$Income >=200000),]

## birth year
data.clean <- data.clean[!(data.clean$Year_Birth <=1901),]
```

```{r convert some variables to categorical variables}

# converting 'Education', 'Marital status', and 'Dt_Customer' variables to categorical variables
## education - creating 3 categories
data.clean$Education[data.clean$Education == "Basic"] <- "1"
data.clean$Education[data.clean$Education == "Graduation"] <- "2"
data.clean$Education[data.clean$Education == "2n Cycle"] <- "3"
data.clean$Education[data.clean$Education == "Master"] <- "3"
data.clean$Education[data.clean$Education == "PhD"] <- "3"
data.clean$Education <- as.numeric(data.clean$Education)

## marital Status - creating 2 categories of Single and together
data.clean$Marital_Status[data.clean$Marital_Status == "Absurd"] <- 0
data.clean$Marital_Status[data.clean$Marital_Status == "Alone"] <- 0
data.clean$Marital_Status[data.clean$Marital_Status == "YOLO"] <- 0
data.clean$Marital_Status[data.clean$Marital_Status == "Single"] <- 0
data.clean$Marital_Status[data.clean$Marital_Status == "Together"] <- 1
data.clean$Marital_Status[data.clean$Marital_Status == "Married"] <- 1
data.clean$Marital_Status[data.clean$Marital_Status == "Divorced"] <- 0
data.clean$Marital_Status[data.clean$Marital_Status == "Widow"] <- 0
data.clean$Marital_Status <- as.numeric(data.clean$Marital_Status)

## Dt_customer
data.clean$Dt_Customer <- dmy(data.clean$Dt_Customer)
data.clean$Dt_Customer <- year(data.clean$Dt_Customer)
data.clean$Dt_Customer <- as.factor(data.clean$Dt_Customer)

data.clean$Dt_Customer <- fct_collapse(data.clean$Dt_Customer,
                                      "3" = "2014",
                                      "1" = "2012",
                                      "2" = "2013")

data.clean$Dt_Customer <- as.numeric(levels(data.clean$Dt_Customer))[data.clean$Dt_Customer]
```

```{r add age variable}
# adding Age variable
data.clean$Age <- (2015 - data.clean$Year_Birth)
```

```{r filter the data which we need and turn them into numeric variables}
# filter numerical columns
numerical.data = data.clean[, c(3:21)]#:ncol(data.ori))]
str(numerical.data)
```

## II. Exploratory Data Analysis
We are going to explore the data as follows:

1. Show the correlation between the variables that have a correlation > Â±0.5

2. Plot the histogram

    
```{r correlation between the variables}
par(mfrow=c(1,1))
corr_simple <- function(data=numerical.data,sig=0.5){
  # convert data to numeric in order to run correlations
  # convert to factor first to keep the integrity of the data - each value will become a number rather than turn into NA
  df_cor <- data %>% mutate_if(is.character, as.factor)
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  # run a correlation and drop the insignificant ones
  corr <- cor(df_cor)
  # prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  # drop perfect correlations
  corr[corr == 1] <- NA 
  # turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  # remove the NA values from above 
  corr <- na.omit(corr) 
  # select significant values  
  corr <- subset(corr, abs(Freq) > sig) 
  # sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  # print table
  print(corr)
  # turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  # plot correlations visually
  corrplot(mtx_corr, is.corr=T, tl.col="black", method = 'number', na.label=" ", number.cex=0.8 ,tl.cex=0.8)
}
corr_simple()
```

```{r plot the histogram}
par(mfrow=c(4,5))
hist(data.clean$Education, main="Education", xlab=" ")
hist(data.clean$Marital_Status, main="Marital_Status", xlab=" ", ylab = " ")
hist(data.clean$Income, main="Income", xlab=" ", ylab = " ")
hist(data.clean$Kidhome, main="Kidhome", xlab=" ", ylab = " ")
hist(data.clean$Teenhome, main="Teenhome", xlab=" ", ylab = " ")
hist(data.clean$Dt_Customer, main="Dt_Customer", xlab=" ")
hist(data.clean$Recency, main="Recency", xlab=" ", ylab = " ")
hist(data.clean$MntWines, main="MntWines", xlab=" ", ylab = " ")
hist(data.clean$MntFruits, main="MntFruits", xlab=" ", ylab = " ")
hist(data.clean$MntMeatProducts, main="MntMeat", xlab=" ", ylab = " ")
hist(data.clean$MntFishProducts, main="MntFish", xlab=" ")
hist(data.clean$MntSweetProducts, main="MntSweet", xlab=" ", ylab = " ")
hist(data.clean$MntGoldProds, main="MntGold", xlab=" ", ylab = " ")
hist(data.clean$NumDealsPurchases, main="NumDeals", xlab=" ", ylab = " ")
hist(data.clean$NumWebPurchases, main="NumWeb", xlab=" ", ylab = " ")
hist(data.clean$NumCatalogPurchases, main="NumCatalog", xlab=" ")
hist(data.clean$NumStorePurchases, main="NumStore", xlab=" ", ylab = " ")
hist(data.clean$NumWebVisitsMonth, main="NumWebVisits", xlab=" ", ylab = " ")
hist(data.clean$Age, main="Age", xlab=" ", ylab = " ")


par(mfrow=c(1,1))
features.order = c("Education", "Marital_Status", "Kidhome", "Teenhome", "Age", 
                   "Dt_Customer", "Income", "Recency", "MntWines", "MntFruits", 
                   "MntMeatProducts", "MntFishProducts", "MntSweetProducts",
                   "MntGoldProds", "NumDealsPurchases", "NumWebPurchases", 
                   "NumCatalogPurchases", "NumStorePurchases", "NumWebVisitsMonth")
```

## III. Dimensionality Reduction
In this part, we are going to compare the dimensionality reduction techniques (eg. FA, PCA and ICA) and see which one is the best.

### 1. FA

Here is the procedure for FA:

1. Use eigenvalues to find the number of principal components.

2. Use factanal function to calculate the final result.

3. Create a new data frame with the selected principal components for heatmap.

We are not supposed to use Pearson correlation as not all variables are numerical. Instead, we should use polychroic correlation. However, because the polychroic correlation is computational expensive, when we apply this algorithm into the data, we will still get the Pearson correlation.
```{r get the eigenvalues and eigenvectors and decide the number of components}

# get the polychroic correlation
fa.cor = POLYCHORIC_R(numerical.data)

# look at eigenvector of the correlation to see how many factors we use
fa.eigen = eigen(fa.cor)
fa.eigen$values
sum(fa.eigen$values)

# to select number of factors
cumsum(fa.eigen$values) / ncol(numerical.data)

# use the scree plot to determine the optimal number of components
plot(fa.eigen$values, type = "b", ylab = "Eigenvalues", xlab = "Factor") 
```

```{r calculate the final result}
# factor analysis with rotation
fa = factanal(x = numerical.data, factors = 6, rotation = "promax", scores = "Bartlett")
head(fa$scores)

# show the result
summary(lm(Factor2 ~ Factor1, data = as.data.frame(fa$scores)))
fa.loading  = data.frame(fa$loadings[1:19,1:6])
fa.loading$features = rownames(fa.loading)
colnames(fa.loading) = c('F1','F2','F3','F4', 'F5', 'F6','features')
```

```{r select components(fa)}
# create a new data frame with the selected principal components
# prepare data for heat map
fa.df = melt(fa.loading)
fa.df$features = factor(fa.df$features, levels = c("Education","Marital_Status","Kidhome","Teenhome", "Age", "Dt_Customer", "Income", "Recency", "MntWines", "MntFruits","MntMeatProducts", "MntFishProducts", "MntSweetProducts","MntGoldProds","NumDealsPurchases", "NumWebPurchases","NumCatalogPurchases", "NumStorePurchases", "NumWebVisitsMonth"))
```

### 2. PCA

1. Apply PCA to the data.

2. Decide the number of principal components.

3. Use the PCA function to calculate the final result.

```{r apply PCA}
# apply PCA to data
pc.res = prcomp(numerical.data, scale = TRUE)
pc.res

```

```{r decide the number of principal components}
# we pick 9 components to explain 80% of variance
cumsum(pc.res$sdev^2/sum(pc.res$sdev^2))
pc.loading = data.frame(pc.res$rotation[1:19,1:9])
pc.loading
```


```{r select components(pca)}
# prepare data for heat map
pc.loading$features = rownames(pc.loading)
pr.loading.df = melt(pc.loading)
pr.loading.df$features = factor(pr.loading.df$features, levels = c("Education", "Marital_Status", "Kidhome", "Teenhome", "Age", 
                                                           "Dt_Customer", "Income", "Recency", "MntWines", "MntFruits", 
                                                           "MntMeatProducts", "MntFishProducts", "MntSweetProducts",
                                                           "MntGoldProds", "NumDealsPurchases", "NumWebPurchases", 
                                                           "NumCatalogPurchases", "NumStorePurchases", "NumWebVisitsMonth"))

```

### 3. ICA

1. Apply ICA to the data.

2. Use the ICA function to calculate the final result.

```{r apply ICA}
# select 6 components since it can capture variables features very well
set.seed(20)
ica = fastICA(numerical.data, 6, fun = "logcosh", alpha = 1,
                   row.norm = T, maxit = 200,
                   tol = 0.0001, verbose = FALSE)
ica.latent =  ica$S
```

```{r select components(ica)}
ica.loading = data.frame(t(ica$A)) %>% 
  rename_with(~ str_glue("IC{seq(.)}")) %>%
  mutate(variable = names(numerical.data)) %>%
  pivot_longer(cols = starts_with("IC"), names_to = "components", values_to = "loading")

# prepare data for heat map
colnames(ica.loading) = c("features","variable","value")
ica.loading$features = factor(ica.loading$features, levels = c("Education", "Marital_Status", "Kidhome", "Teenhome", "Age", 
                                                              "Dt_Customer", "Income", "Recency", "MntWines", "MntFruits", 
                                                              "MntMeatProducts", "MntFishProducts", "MntSweetProducts",
                                                              "MntGoldProds", "NumDealsPurchases", "NumWebPurchases", 
                                                              "NumCatalogPurchases", "NumStorePurchases", "NumWebVisitsMonth"))

# output the loadings
t(ica$A)
```



### 4. Compare the components of three methods

```{r compare the components}

fa.plot = ggplot(fa.df, aes(x = variable, y = factor(features, level = features.order), fill = value)) +
  geom_tile(color = "white",
            lwd = 1.5,
            linetype = 1, show.legend = FALSE) +
  scale_fill_gradient2(low='blue',high='red') +
  ylab("Features") + xlab("Factors") + ggtitle('FA') +
  coord_fixed() + theme(plot.title = element_text(hjust = 0.5))

pr.plot = ggplot(pr.loading.df, aes(x = variable, y = factor(features, level = features.order), fill = value)) +
  geom_tile(color = "white",
            lwd = 1.5,
            linetype = 1, show.legend = FALSE) +
  scale_fill_gradient2(low='blue',high='red') + xlab("Components") + ggtitle('PCA') +
  coord_fixed() + ylab(NULL) + 
  theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), plot.title = element_text(hjust = 0.5))

ica.plot = ggplot(ica.loading, aes(x = variable, y = factor(features, level = features.order), fill = value)) +
  geom_tile(color = "white", lwd = 1.5, linetype = 1) + 
  scale_fill_gradient2(low='blue',high='red',limits=c(-1,1)) + xlab("Components") + ggtitle('ICA') +
  coord_fixed() + ylab(NULL) + 
  theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), plot.title = element_text(hjust = 0.5))

# heat map comparison plot
grid.arrange(fa.plot, pr.plot, ica.plot, nrow = 1, 
             top=textGrob("Compare loadings among FA, PCA and ICA", gp=gpar(fontsize=15, font = 2)))
```

## IV. Clustering
We are going to use KMeans clustering to cluster the data after applying ICA.
1. Select the number of clusters based on the elbow plot.
2. Apply KMeans clustering to the data.

```{r elbow plot}
# elbow plot
fviz_nbclust(ica.latent, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2) + # add line for better visualisation
  labs(subtitle = "Elbow method") # add subtitle
```

We decide to use 4 clusters.

```{r apply KMeans}
kmean.model.4 = kmeans(ica.latent, 4, nstart = 10)
kmean.model.4.cluster = kmean.model.4$cluster
# have a brief look at the cluster
# define a function to do min max scale
min.max.scale <- function(x){(x-min(x))/(max(x)-min(x))}
scatterplot3d(min.max.scale(ica.latent[,1]),min.max.scale(ica.latent[,2]),min.max.scale(ica.latent[,3]),color = kmean.model.4.cluster, main='Clusters in 3 dimensions',xlab ='Dimension 1',ylab= '', zlab= 'Dimension 3',scale.y = 1)


text(x = 7.5, y = 0.5, "Dimension 2", srt = 45)
```

## V. Access the model

We are going to assess the distribution of these four clusters through two ways.
1. Use RFM to assess the model.
2. Assess the model through features.

```{r RFM analysis}

# create new dataset for RFM
rfm.R = data.clean$Recency
rfm.F = rowSums(data.clean[,16:20])
rfm.M = rowSums(data.clean[,10:15])
rfm.income = data.clean$Income
RFM = data.frame(rfm.R,rfm.F,rfm.M,rfm.income)

RFM$rfm.R.scaled = min.max.scale(RFM$rfm.R)
RFM$rfm.F.scaled = min.max.scale(RFM$rfm.F)
RFM$rfm.M.scaled = min.max.scale(RFM$rfm.M)
RFM$rfm.income.scaled = min.max.scale(RFM$rfm.income)

RFM$cluster = kmean.model.4.cluster


# plot RFM with different clusters
RFM%>%
  select(cluster, rfm.R.scaled, rfm.F.scaled, rfm.M.scaled,rfm.income.scaled)%>%
  melt(id='cluster')%>%
  ggplot(aes(as_factor(cluster), value, fill=as.factor(cluster)))+
  scale_fill_manual('Cluster',values=c("#ff8c7a", "#92C5DE", "yellow", "#B8E186"))+
  geom_boxplot()+
  facet_wrap(~variable, ncol = 4)+
  labs(x = "Cluster", labs="Cluster")
```

```{r feature distribution versus clusters}
# plot the features vs different clusters
data.scaled <- data.clean

data.scaled <- data.frame(sapply(data.scaled, min.max.scale))
data.scaled$cluster <- RFM$cluster

data.scaled%>%
  select(cluster, Age, Kidhome, Teenhome,Education, Marital_Status,
         MntWines, MntFruits, MntMeatProducts, MntFishProducts, 
         MntSweetProducts, MntGoldProds, NumDealsPurchases, NumWebPurchases,
         NumCatalogPurchases, NumStorePurchases, NumWebVisitsMonth)%>%
  melt(id='cluster')%>%
  ggplot(aes(as_factor(cluster), value, fill=as.factor(cluster)))+
  scale_fill_manual(values=c("#ff8c7a", "#92C5DE", "yellow", "#B8E186"))+
  geom_boxplot()+
  facet_wrap(~variable, ncol = 4) + 
  theme_light() +
  labs(x = "Cluster", fill = "Cluster") +
  scale_color_manual(name="Cluster",values=c("1","2","3","4"))
```